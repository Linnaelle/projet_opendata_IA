# NutriScan Configuration
# Copy this file to .env and configure your settings

# ===== LLM Provider Selection =====
# Choose which AI provider to use: openai, gemini, or ollama
NUTRISCAN_PROVIDER=ollama

# ===== OpenAI Configuration =====
# Required if using provider=openai
OPENAI_API_KEY=your_openai_key_here
NUTRISCAN_MODEL_OPENAI=gpt-4o-mini

# ===== Google Gemini Configuration =====
# Required if using provider=gemini
GEMINI_API_KEY=your_gemini_key_here
NUTRISCAN_MODEL_GEMINI=gemini/gemini-2.0-flash-exp

# ===== Ollama Configuration (Local) =====
# Required if using provider=ollama
# Make sure Ollama is running: ollama serve
NUTRISCAN_MODEL_OLLAMA=ollama/mistral
OLLAMA_API_BASE=http://localhost:11434
